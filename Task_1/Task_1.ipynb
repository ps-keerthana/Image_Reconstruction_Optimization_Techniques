{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 : Ascending the Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the below dataset for Task 1:\n",
    "\n",
    "```python\n",
    "np.random.seed(45)\n",
    "num_samples = 40\n",
    "\n",
    "# Generate data\n",
    "x1 = np.random.uniform(-1, 1, num_samples)\n",
    "f_x = 3*x1 + 4\n",
    "eps = np.random.randn(num_samples)\n",
    "y = f_x + eps\n",
    "```\n",
    "\n",
    "1. Use `torch.autograd` to find the true gradient on the above dataset using linear regression (in the form \n",
    "$\\theta_1 x + \\theta_0$) for any given values of $(\\theta_0, \\theta_1)$.\n",
    "\n",
    "2. Using the same $(\\theta_0, \\theta_1)$ as above, calculate the stochastic gradient for all points in the dataset. Then, find the average of all those gradients and show that the stochastic gradient is a good estimate of the true gradient. \n",
    "\n",
    "3. Implement full-batch, mini-batch, and stochastic gradient descent. Calculate the average number of iterations required for each method to get sufficiently close to the optimal solution, where \"sufficiently close\" means within a distance of \n",
    "$\\epsilon$ (or $\\epsilon$-neighborhood) from the minimum value of the loss function. Visualize the convergence process for 15 epochs. Choose $\\epsilon = 0.001$ for convergence criteria.  \n",
    "Which optimization process takes a larger number of epochs to converge, and why?  \n",
    "Show the contour plots for different epochs (or show an animation/GIF) for visualization of the optimization process. Also, make a plot for Loss vs. Epochs for all the methods. \n",
    "\n",
    "4. Explore the article [here](https://machinelearningmastery.com/gradient-descent-with-momentum-from-scratch/#:~:text=Momentum%20is%20an%20extension%20to,spots%20of%20the%20search%20space.)  on gradient descent with momentum. Implement gradient descent with momentum for the dataset. Visualize the convergence process for 15 steps. Compare the average number of steps taken with gradient descent (for variants full-batch and stochastic) with momentum to that of vanilla gradient descent to converge to an $\\epsilon$-neighborhood for both datasets. Choose $\\epsilon = 0.001$.  \n",
    "Write down your observations. Show the contour plots for different epochs for momentum implementation. Specifically, show all the vectors: gradient, current value of $\\theta$, momentum, etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Using autograd to find the true gradient on the above dataset using linear regression for any  $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "np.random.seed(45)\n",
    "x_vals = np.random.uniform(-1, 1, 40)\n",
    "y_vals = 3 * x_vals + 4 + np.random.randn(40)\n",
    "\n",
    "# Convert to tensors\n",
    "x = torch.tensor(x_vals, dtype=torch.float64).view(-1, 1)\n",
    "y = torch.tensor(y_vals, dtype=torch.float64).view(-1, 1)\n",
    "\n",
    "# Design matrix with bias\n",
    "ones_col = torch.ones_like(x)\n",
    "X = torch.cat((ones_col, x), dim=1)  # shape (40, 2)\n",
    "\n",
    "# Initialize θ = [1.0, 2.0] for autograd\n",
    "theta = torch.tensor([1.0, 2.0], dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "# Predictions and Loss\n",
    "y_hat = X @ theta.view(-1, 1)\n",
    "loss = torch.mean((y_hat - y) ** 2)\n",
    "\n",
    "# Gradient via autograd\n",
    "loss.backward()\n",
    "print(\"Initial θ:\", theta.detach().numpy())\n",
    "print(\"Gradient at θ = [1, 2]:\", theta.grad.numpy())\n",
    "print(\"Loss at θ = [1, 2]:\", loss.item())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute optimal θ using normal equation\n",
    "X_detached = X.detach()\n",
    "y_detached = y.detach()\n",
    "theta_opt = torch.inverse(X_detached.T @ X_detached) @ X_detached.T @ y_detached\n",
    "\n",
    "# Calculate loss at optimal θ\n",
    "y_pred_opt = X_detached @ theta_opt\n",
    "optimal_loss = torch.mean((y_pred_opt - y_detached) ** 2).item()\n",
    "\n",
    "print(\"Optimal θ (closed-form):\", theta_opt.view(-1).numpy())\n",
    "print(\"Minimum possible Loss (MSE):\", optimal_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Using the same $(\\theta_0, \\theta_1)$ as above, and calculate the stochastic gradient for all points in the dataset. Then, find the average of all those gradients and show that the stochastic gradient is a good estimate of the true gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the true gradient \n",
    "def get_true_grad(X, y, theta):\n",
    "    if theta.grad is not None:\n",
    "        theta.grad.zero_()\n",
    "\n",
    "    preds = X @ theta.view(-1, 1)\n",
    "    loss = torch.mean((preds - y) ** 2)\n",
    "    loss.backward()\n",
    "    return theta.grad.clone(), loss.item()\n",
    "\n",
    "true_grad, true_loss = get_true_grad(X, y, theta)\n",
    "\n",
    "# Function to compute gradient on a single point (stochastic)\n",
    "def grad_on_one_point(x_single, y_single):\n",
    "    if theta.grad is not None:\n",
    "        theta.grad.zero_()\n",
    "    \n",
    "    pred = x_single @ theta.view(-1, 1)\n",
    "    loss = (pred - y_single) ** 2  # no mean, since it's one point\n",
    "    loss.backward()\n",
    "    return theta.grad.clone()\n",
    "\n",
    "# Loop over each point and collect gradients\n",
    "sgd_list = []\n",
    "\n",
    "for i in range(len(y)):\n",
    "    xi = X[i].view(1, -1).detach()\n",
    "    yi = y[i].view(1, -1).detach()\n",
    "    grad_i = grad_on_one_point(xi, yi)\n",
    "    sgd_list.append(grad_i)\n",
    "\n",
    "# Average all gradients\n",
    "avg_sgd = torch.stack(sgd_list).mean(dim=0)\n",
    "\n",
    "# Compare with true gradient\n",
    "residual = true_grad - avg_sgd\n",
    "\n",
    "# Final outputs\n",
    "print(\"True Gradient:\", true_grad.numpy())\n",
    "print(\"Avg. Stochastic Gradient:\", avg_sgd.numpy())\n",
    "print(\"Difference (Residual):\", residual.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculated the gradient using each individual data point (stochastic) and then averaged them. The resulting average matched the gradient computed using the entire dataset. The residual difference was extremely small (~1e-16), which confirms that the stochastic gradient is a reliable approximation of the full gradient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3,4. Implementing full-batch, mini-batch, and stochastic gradient descent to minimize a loss function, computing average steps to reach an ϵ-neighborhood (ϵ = 0.001), and visualizing convergence over 15 epochs using loss plots. Extending to gradient descent with momentum (full-batch and stochastic), plotting contour plots across epochs, comparing with vanilla GD, and noting observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Data Setup\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(45)\n",
    "num_samples = 40\n",
    "x_vals = np.random.uniform(-1, 1, num_samples)\n",
    "y_vals = 3 * x_vals + 4 + np.random.randn(num_samples)\n",
    "\n",
    "x = torch.tensor(x_vals, dtype=torch.float64).view(-1, 1)\n",
    "y = torch.tensor(y_vals, dtype=torch.float64).view(-1, 1)\n",
    "X = torch.cat([torch.ones_like(x), x], dim=1)\n",
    "\n",
    "# Compute Optimal Theta\n",
    "\n",
    "theta_opt = torch.linalg.inv(X.T @ X) @ X.T @ y\n",
    "loss_opt = torch.mean((X @ theta_opt - y) ** 2).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRadient Descent Function\n",
    "\n",
    "def gradient_descent(method='batch', lr=0.05, eps=1e-3, max_epochs=2000, batch_size=8, momentum=None):\n",
    "    theta = torch.randn(2, dtype=torch.float64, requires_grad=True)\n",
    "    prev_update = torch.zeros_like(theta)\n",
    "\n",
    "    losses = []\n",
    "    thetas = []\n",
    "    grads = []\n",
    "    momentums = []\n",
    "\n",
    "    iter_count = 0  # Track number of iterations (parameter updates)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        if method == 'batch':\n",
    "            batches = [(X, y)]\n",
    "        elif method == 'mini':\n",
    "            idx = torch.randperm(num_samples)\n",
    "            X_shuff = X[idx].detach()\n",
    "            y_shuff = y[idx].detach()\n",
    "            batches = [(X_shuff[i:i+batch_size], y_shuff[i:i+batch_size]) for i in range(0, num_samples, batch_size)]\n",
    "        elif method == 'stochastic':\n",
    "            idx = torch.randperm(num_samples)\n",
    "            X_shuff = X[idx].detach()\n",
    "            y_shuff = y[idx].detach()\n",
    "            batches = [(X_shuff[i:i+1], y_shuff[i:i+1]) for i in range(num_samples)]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid method\")\n",
    "\n",
    "        total_loss = 0\n",
    "        for xb, yb in batches:\n",
    "            iter_count += 1  #  Count iteration per batch\n",
    "            theta = theta.detach().requires_grad_(True)\n",
    "            pred = xb @ theta.view(-1, 1)\n",
    "            loss = torch.mean((pred - yb)**2)\n",
    "            loss.backward()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                grad = theta.grad\n",
    "                if momentum:\n",
    "                    update = lr * grad + momentum * prev_update\n",
    "                    prev_update = update\n",
    "                else:\n",
    "                    update = lr * grad\n",
    "                theta -= update\n",
    "\n",
    "            grads.append(grad.clone().detach())\n",
    "            momentums.append(prev_update.clone().detach())\n",
    "            theta. grad.zero_()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(batches)\n",
    "        losses.append(avg_loss)\n",
    "        thetas.append(theta.detach().clone())\n",
    "\n",
    "        if torch.norm(theta - theta_opt.squeeze()) <= eps:\n",
    "            break\n",
    "\n",
    "    return losses, thetas, grads, momentums, epoch + 1, iter_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss vs Epochs\n",
    "\n",
    "def plot_losses(momentum=None, epoch_cap=None):\n",
    "    methods = {'batch': 'Batch GD', 'mini': 'Mini-Batch GD', 'stochastic': 'Stochastic GD'}\n",
    "    results = {}\n",
    "\n",
    "    for method in methods:\n",
    "        losses, _, _, _, epochs_run, iterations = gradient_descent(method=method, momentum=momentum)\n",
    "        results[method] = (epochs_run, iterations)\n",
    "        if epoch_cap is None:\n",
    "            x_vals = range(1, len(losses)+1)\n",
    "            y_vals = losses\n",
    "        else:\n",
    "            x_vals = range(1, min(epoch_cap, len(losses)) + 1)\n",
    "            y_vals = losses[:min(epoch_cap, len(losses))]\n",
    "        \n",
    "        plt.plot(x_vals, y_vals, label=methods[method])\n",
    "\n",
    "    plt.axhline(y=loss_opt, linestyle='--', color='red', label='Optimal Loss')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss vs Epochs (ε = 0.001)\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Epochs & Iterations until convergence (ε = 0.001):\")\n",
    "    for m in methods:\n",
    "        e, it = results[m]\n",
    "        print(f\"{methods[m]}: {e} epochs, {it} iterations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Epochs Over Trials\n",
    "\n",
    "def average_epochs_and_iterations(method, trials=10, momentum=None):\n",
    "    epoch_counts = []\n",
    "    iter_counts = []\n",
    "    for seed in range(trials):\n",
    "        torch.manual_seed(seed)\n",
    "        _, _, _, _, epochs, iterations = gradient_descent(method=method, momentum=momentum)\n",
    "        epoch_counts.append(epochs)\n",
    "        iter_counts.append(iterations)\n",
    "    return np.mean(epoch_counts), np.mean(iter_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Momentum vs No Momentum\n",
    "\n",
    "def compare_momentum():\n",
    "    print(\"\\nAverage over 10 trials WITHOUT momentum:\")\n",
    "    for method in ['batch', 'mini', 'stochastic']:\n",
    "        avg_ep, avg_it = average_epochs_and_iterations(method, momentum=None)\n",
    "        print(f\"{method.title():<10}: {avg_ep:.2f} epochs, {avg_it:.2f} iterations\")\n",
    "\n",
    "    print(\"\\nAverage over 10 trials WITH momentum:\")\n",
    "    for method in ['batch', 'mini', 'stochastic']:\n",
    "        avg_ep, avg_it = average_epochs_and_iterations(method, momentum=0.9)\n",
    "        print(f\"{method.title()} + Momentum: {avg_ep:.2f} epochs, {avg_it:.2f} iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(momentum=None, epoch_cap=15)\n",
    "plot_losses(momentum=0.9, epoch_cap=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_momentum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations\n",
    "\n",
    "1. **Stochastic Gradient Descent (SGD)** took the **most epochs and iterations** to converge — around **1798 epochs (without momentum)** and **1843 epochs (with momentum)** on average.  \n",
    "   This is because it updates parameters using only one data point at a time, which causes a lot of fluctuation (noise) in the gradient direction, making it slower to settle near the minimum.  \n",
    "   \n",
    "\n",
    "2. **Batch Gradient Descent** was the **most stable and efficient** method. It used the full dataset for each update, resulting in smooth convergence.  \n",
    "   When momentum was added, the performance improved a lot, average epochs reduced from **258.5 (vanilla)** to just **102 (with momentum)**.  \n",
    "  \n",
    "\n",
    "3. **Mini-Batch Gradient Descent** performed well **without momentum**, requiring only **94 epochs** on average.  \n",
    "   But with momentum, its performance **got worse**, taking **1725.4 epochs** to converge.  \n",
    "   This is likely because the randomness in mini-batches, combined with momentum, may have led to overshooting or instability.  \n",
    "   \n",
    "\n",
    "4. In general, **momentum is useful when updates are consistent** (like in batch GD), but **less helpful or even harmful** when updates are noisy (like in mini/stochastic GD).  \n",
    "   Momentum works best when the directionof updatess doesn’t vary too much; otherwise, it can add to the instability instead of speeding things up.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "def generate_contour_animation(thetas, gif_name='gd_animation.gif'):\n",
    "    w0 = np.linspace(-2, 6, 100)\n",
    "    w1 = np.linspace(-4, 10, 100)\n",
    "    W0, W1 = np.meshgrid(w0, w1)\n",
    "    Z = np.zeros_like(W0)\n",
    "\n",
    "    for i in range(W0.shape[0]):\n",
    "        for j in range(W0.shape[1]):\n",
    "            w = torch.tensor([W0[i, j], W1[i, j]], dtype=torch.float64)\n",
    "            y_pred = X @ w.view(-1, 1)\n",
    "            Z[i, j] = torch.mean((y - y_pred)**2).item()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    ax.contour(W0, W1, Z, levels=30, cmap='viridis')\n",
    "    ax.set_xlabel(\"Theta 0\")\n",
    "    ax.set_ylabel(\"Theta 1\")\n",
    "    ax.set_title(\"Gradient Descent Path Animation\")\n",
    "\n",
    "    # Thin red line with small transparent markers\n",
    "    path, = ax.plot([], [], color='red', linewidth=1, marker='o', markersize=3, alpha=0.6)\n",
    "\n",
    "    theta_np = np.array([[t[0].item(), t[1].item()] for t in thetas])\n",
    "\n",
    "    def init():\n",
    "        path.set_data([], [])\n",
    "        return path,\n",
    "\n",
    "    def animate(i):\n",
    "        path.set_data(theta_np[:i+1, 0], theta_np[:i+1, 1])\n",
    "        return path,\n",
    "\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, animate, init_func=init, frames=len(thetas),\n",
    "        interval=300, blit=True\n",
    "    )\n",
    "\n",
    "    anim.save(gif_name, writer='pillow')\n",
    "    plt.close()\n",
    "    print(f\"Saved animation to {gif_name}\")\n",
    "    return HTML(anim.to_jshtml())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_, thetas, _, _, _,_ = gradient_descent(method='batch', momentum=0.9)\n",
    "generate_contour_animation(thetas, gif_name=\"batch_mmntm_gd.gif\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_, thetas, _, _, _,_ = gradient_descent(method='batch', momentum=None)\n",
    "generate_contour_animation(thetas, gif_name=\"batch_gd.gif\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_, thetas, _, _, _,_ = gradient_descent(method='mini', momentum=0.9)\n",
    "generate_contour_animation(thetas, gif_name=\"minibatch_mmntm_gd.gif\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_, thetas, _, _, _, _ = gradient_descent(method='mini', momentum=None)\n",
    "generate_contour_animation(thetas, gif_name=\"minibatch_gd.gif\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_, thetas, _, _, _,_ = gradient_descent(method='stochastic', momentum=0.9)\n",
    "generate_contour_animation(thetas, gif_name=\"stochastic_mmntm_gd.gif\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_, thetas, _, _, _, _ = gradient_descent(method='stochastic', momentum=None)\n",
    "generate_contour_animation(thetas, gif_name=\"stochastic_gd.gif\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
